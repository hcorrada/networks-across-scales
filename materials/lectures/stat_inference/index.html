<!DOCTYPE html>
<html>
  <head>
    <title>Statistical Analysis of Network Data</title>
    <meta charset="utf-8">
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2018-10-29" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide, center, middle
count: false

.banner[![](img/head.png)]

.title[Statistical Analysis of Network Data]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC828O 2018-10-29
]

.logo[![](img/logo.png)]

---
class: split-50
exclude: true

## What does my group do?

.column[
Study the **molecular** basis of *variation* in development and disease


Using **high-throughput** experimental methods  
]

.column[.image-80[![](img/stickmen.png)]]

---
layout: true

## Statistical Analysis

---

In this next unit we will look at methods that approach network analysis from a statistical inference perspective. 

---

In particular we will look at three statistical inference and learning tasks over networks

- Analyzing edges between vertices as a stochastic process over which we can make statistical inferences

- Constructing networks from observational data

- Analyzing a process (e.g., diffusion) over a network in a statistical manner

---
layout: false

## Spatial effects in ecological networks

.center.middle[![](img/spatial.png)]

.source[https://doi.org/10.1016/j.prevetmed.2014.01.013]



---
layout: true

## Exponential Random Graph Models

---

In ER random graph model edge probabilities were independent of vertex characterisitics.

Now assume vertices have measured attributes. 

Question: what is the effect of these attributes in network formation, specifically in edge occurence.

---

Denote `\(Y\)` as adjacency matrix of graph `\(G\)` over `\(n\)` elements

Denote `\(X\)` as matrix of vertex attributes.

We want to determine `\(P(Y_{ij}=1|Y_{-(ij)}=y_{-(ij)}, x_i, x_j, L(G))\)`

where `\(L(G)\)` is a measure of structure of graph `\(G\)` and `\(y_{-(ij)}\)` is the _configuration_ of edges other than edge `\(i\sim j\)`

---
class: split-50

We can motivate ERGM model from regression (where outcome `\(Y\)` is continuous)

.column[
`$$E[Y_i|x_i] = \sum_{j=1}^p \beta_j x_{ij} = \beta'x_i$$`
]

.column[
.center[![](img/lr.png)]
]

---
class: split-50

We turn into a probabilistic model as

.column[
`$$Y=\beta'x_i + \epsilon$$`

`$$\epsilon \sim N(0,\sigma^2)$$`
]

.column[
.center[![](img/lr.png)]
]

---

For _binary_ outcome `\(Y\)` we use _logistic regression_

.center[![](img/logis.png)]

`$$\log\frac{P(Y_i=1|x_i)}{1-P(Y_i=1|x_i)} = \beta'x_i$$`

Which corresponds to a Bernoulli model of `\(P(Y_i=1|x_i)\)`.

---

The outcome of interest in the ERGM model is the _presence_ of edge `\(y_{ij}=1\)`.

Use a Bernoulli model with `\(y_{ij}\)` as the outcome.

With vertex attributes and graph structural measure as predictors.

---

Model 1: the ER model

`$$\log \frac{P(Y_{ij}=1|Y_{-(ij)}=y_{-(ij)})}{P(Y_{ij}=0|Y_{-(ij)}=y_{-(ij)})} = \theta$$`

Thinking of logistic regression: model is a _constant_, independent of rest of graph structure, independent of vertex attributes

---

To fit models we need a _likelihood_, i.e., probability of observed graph, given parameters (in this case `\(\theta\)`)

--

Write `\(P(Y_{ij}=1|...)\)` as `\(p\)`, then likelihood is given by

`$$\mathcal{L}(\theta;y) = \prod_{ij} p^{y_{ij}}(1-p)^{(1-y_{ij})}$$`
---

(Exercise)

`$$\mathcal{L}(\theta;y)=\frac{1}{\kappa}\exp\{\theta L(y)\}$$`

where `\(L(y)\)` is the number of edges in the graph.

This is the formulation given in reading!

---
class: split-30

.column[

```
## Observations: 36
## Variables: 9
## $ name      &lt;chr&gt; "V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9"...
## $ Seniority &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1...
## $ Status    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ Gender    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ Office    &lt;int&gt; 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 2...
## $ Years     &lt;int&gt; 31, 32, 13, 31, 31, 29, 29, 28, 25, 25, 23, 24, 22, ...
## $ Age       &lt;int&gt; 64, 62, 67, 59, 59, 55, 63, 53, 53, 53, 50, 52, 57, ...
## $ Practice  &lt;int&gt; 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1...
## $ School    &lt;int&gt; 1, 1, 1, 3, 2, 1, 3, 3, 1, 3, 1, 2, 2, 1, 3, 1, 1, 2...
```
]

&lt;img src="index_files/figure-html/unnamed-chunk-4-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
class: split-70

.column[

```r
library(ergm)
A &lt;- get.adjacency(lazega)
lazega.s &lt;- network::as.network(as.matrix(A), directed=FALSE)
ergm.bern.fit &lt;- ergm(lazega.s ~ edges)
ergm.bern.fit
```

```
## 
## MLE Coefficients:
##  edges  
## -1.499
```
]

.column[


So `\(\theta=-1.5\)`

and thus `\(p=0.183\)`
]


---



&lt;img src="index_files/figure-html/unnamed-chunk-8-1.png" width="1152" style="display: block; margin: auto;" /&gt;&lt;img src="index_files/figure-html/unnamed-chunk-8-2.png" width="1152" style="display: block; margin: auto;" /&gt;


---
class: split-50

The ER model is not appropriate, let's extend with more graph statistics.

.column[
![](img/kstar.png)

`\(S_k(y)\)`: number of `\(k\)`-stars
]

.column[
.center[![](img/tris.png)]
`\(T_k(y)\)`: number of `\(k\)`-triangles 
]

---

In practice, instead of adding terms for structural statistics at all values of `\(k\)`, they are combined into a single term

For example _alternating `\(k\)`-star counts_

`$$\mathrm{AKS}_{\lambda}(y) = \sum_{k=2}^{N_v-1} (-1)^k \frac{S_k(y)}{\lambda^{k-2}}$$`

`\(\lambda\)` is a parameter that controls decay of influence of larger `\(k\)` terms. Treat as a hyper-parameter of model

---

Another example is _geometrically weighted degree count_

`$$\mathrm{GWD}_{\gamma}(y) = \sum_{d=0}^{N_v-1} e^{-\gamma d}N_d(y)$$`

There is a good amount of literature on definitions and properties of suitable terms to summarize graph structure in these models

---

In addition we want to adjust edge probabilities based on vertex attributes

For edge `\(i \sim j\)`, `\(i\)` may have attribute that increases degree (e.g., seniority)

Or, `\(i\)` and `\(j\)` have attributes that _together_ increase edge probability (e.g., spatial distance in an ecological network)

---

We can add attribute terms to the ERGM model accordingly. E.g.,

- Main effects: `\(h(x_i,x_j)=x_i + x_j\)`
- Categorical interaction (match): `\(h(x_i,x_j)=I(x_i == x_j)\)`
- Numeric interaction: `\(h(x_i,x_j)=(x_i-x_j)^2\)`

---



A full ERGM model for this data:


```r
lazega.ergm &lt;- formula(lazega.s ~ edges + gwesp(log(3), fixed=TRUE) +
  nodemain("Seniority") +
  nodemain("Practice") +                         
  match("Practice") +
  match("Gender") +
  match("Office"))
```



---


```
## # A tibble: 1 x 5
##   independence iterations logLik   AIC   BIC
##   &lt;lgl&gt;             &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 FALSE                 2  -230.  474.  505.
```

---

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; mcmc.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; edges &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -7.0126525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7030685 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; gwesp.fixed.1.09861228866811 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5906030 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0866729 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; nodecov.Seniority &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0247161 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0063578 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0001013 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; nodecov.Practice &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3960366 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1042741 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0001458 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; nodematch.Practice &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7694967 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1961173 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000872 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; nodematch.Gender &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.7374215 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.2463944 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0027639 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; nodematch.Office &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.1654418 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.1905383 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---


&lt;img src="index_files/figure-html/unnamed-chunk-11-1.png" width="1152" style="display: block; margin: auto;" /&gt;&lt;img src="index_files/figure-html/unnamed-chunk-11-2.png" width="1152" style="display: block; margin: auto;" /&gt;

---

A few more points:

The general formulation for ERGM is

`$$P_{\theta}(Y=y) = \frac{1}{\kappa} \exp \left\{ \sum_H g_H(y) \right\}$$`

where `\(H\)` represents possible _configurations_ of possible edges among a subset of vertices in graph

`\(g_H(y) = \prod_{y_{ij}\in H} y_{ij}=1\)` if configuration `\(H\)` occurs in graph

---

This brings about some complications since it's infeasible to define function over all possible configurations

Instead, collapse configurations into groups based on certain properties, and count the number of times these properties are satisfied in graph

Even then, computing normalization term `\(\kappa\)` is also infeasible, therefore use sampling methods (MCMC) for estimation

---
layout: true

## Stochastic Block Models

---

.center.middle[![](img/sbm.png)]

.source[https://doi.org/10.1371/journal.pbio.1002527]

---

Method to cluster vertices in graph

Assume that each vertex belongs to one of `\(Q\)` classes

Then probability of edge `\(i \sim j\)` depends on class/cluster membership of vertices `\(i\)` and `\(j\)`

---

### Clustered ERGM model

If we knew vertex classes, e.g., `\(i\)` belongs to class `\(q\)` and `\(j\)` belongs to class `\(r\)`

`$$\log\frac{P(Y_{ij}=1|Y_{-(ij)}=y_{-(ij)})}{P(Y_{ij}=0|Y_{-(ij)}=y_{-(ij)})} = \theta_{qr}$$`

---

### Clustered ERGM model

Likelihood is then

`$$\mathcal{L}(\theta;y) = \frac{1}{\kappa} \exp \{ \sum_{qr} \theta_{qr} L_{qr}(y) \}$$`

with `\(L_{qr}(y)\)` the number of edges `\(i \sim j\)` where `\(i\)` in class `\(q\)` and `\(j\)` in class `\(r\)` 

(a model like `g~match(class)` in ERGM)

---

However, suppose we don't know vertex class assignments...

SBM is a probabilistic method where we maximize likelihood of this model, assuming class assignments are unobserved

---

.image-70.center.middle[![](img/gmm.png)]

---

Ingredients:

- `\(Y_{ij}\)` edge `\(i \sim j\)` (binary)
- `\(z_{iq}\)` indicator for vertex `\(i\)` class `\(q\)` (binary)
- `\(\alpha_q\)` prior for class `\(q\)` `\(p(z_{iq}=1) = \alpha_q\)`
- `\(\pi_{qr}\)`: probability of edge `\(i \sim j\)` where `\(i\)` in class `\(q\)` and `\(j\)` in class `\(r\)`

---

With this we can write again a likelihood

`$$\mathcal{L}(\theta;y,z)=\sum_i\sum_q z_{iq} \log \alpha_q + \frac{1}{2} \sum_{i\neq j}\sum_{q\neq r} z_{iq}z_{jr}b(y_{ij};\pi_{qr})$$`

with `\(b(y,\pi)=\pi^y(1-\pi)^{(1-y)}\)`

---

Like similar models (e.g., Gaussian mixture model, Latent Dirichlet Allocation) can't optimize this directly

Instead EM algorithm used:
  - Initialize parameters `\(\theta\)`
  - Repeat until "convergence":
    - Compute `\(\gamma_{iq}=E\{z_{iq} | y; \theta\}=p(z_{iq}| y; \theta)\)`
    - Maximize likelihood w.r.t. `\(\theta\)` plugging in `\(\gamma_{iq}\)` for `\(z_{iq}\)`.

---

Like similar models need to determine number of classes (clusters) and select using some model selection criterion

- AIC
- BIC
- Integrated Classification Likelihood

---

.image-60.center.middle[![](img/sbm_res.png)]

---
layout: true

## Communities

---

If we think of class as _community_ we can see relationship with non-probabilistically community finding methods (e.g., Newman-Girvan)

.center[![](img/ng.png)]

See http://www.pnas.org/content/106/50/21068.full
for comparison of these methods

---
exclude: true
NEWMAN-GIRVAN algorithm

---
exclude: true

COMPARISON TO LATENT AND SBM MODELS

Cite the Bickel paper:
http://www.pnas.org/content/106/50/21068.full


---
layout: false

## Summary

Slightly different way of thinking probabilistically about networks

Define probabilistic model over network configurations

Parameterize model using network structural properties and vertex properties

Perform inference/analysis on resulting parameters

Can also extend classical clustering methodology to this setting

---
exclude: true
layout: true

## Latent Network Models

---
exclude: true

Extension to include attributes.

Drawn from SNA literature to model homophily





---
layout: true

## Learning Network Structure
---

How to find network structure from observational data (e.g., gene expression)
  
.center.image-70[![](img/yeast_struct.png)]

---

### Correlation Networks

The simplest approach: compute correlation between observations, if correlation high, add an edge

---

Assume data `\(y_i=\{y_{i1}, y_{i2}, \ldots, y_{iT}\}\)` (e.g., gene expression of gene `\(i\)` in `\(T\)` different conditions) and `\(y_j\)`

Important quantity 1: the _covariance_ of `\(y_i\)` and `\(y_j\)`

`$$\sigma_{ij} = \frac{1}{T}\sum_{t=1}^T (y_{it}-\overline{y}_i)(y_{jt}-\overline{y}_j)$$`


---

Important quantity 1: the _covariance_ of `\(y_i\)` and `\(y_j\)`

`$$\sigma_{ij} = \frac{1}{T} \sum_{t=1}^T (y_{it}-\overline{y}_i)(y_{jt}-\overline{y}_j)$$`

How do `\(y_i\)` and `\(y_j\)` vary around their means?

---

We can estimate `\(\sigma_{ij}\)` from data by plugging in the mean of `\(y_i\)` and `\(y_j\)`. 

We would notate the estimate as `\(\hat{\sigma}_{ij}\)`. 

In the following, `\(\sigma_{ij}\)` often means `\(\hat{\sigma}_{ij}\)`, it should follow from context.

---

We often need to compare quantities across different entities in system, e.g., genes, so we want to remove _scale_

_Pearson's Correlation_: 

`$$\rho_{ij} = \frac{\sigma_{ij}}{\sigma_{ii}\sigma_{jj}}$$`
With `\(\sigma_{ii}\)` the standard deviation of `\(y_i\)`:

`$$\sigma_{ii} = \sqrt{\frac{1}{T} \sum_{i=1}^T (y_{it}-\overline{y}_i)}$$`

---

Note Pearson Correlation is between -1 and 1, it is hard to perform inference on bounded quantities, so one more transformation.

Fisher's transformation

`$$z{ij} = \tanh^{-1}(\rho_{ij}) = \frac{1}{2}\log \frac{1+\rho_{ij}}{1-\rho_{ij}}$$`

---
class: split-40

.column[
Edge inference: hypothesis test

`$$H_0=\rho_{ij}=0 \; H_A=\rho_{ij} \neq 0$$`

Compute `\(P\)`-value `\(p_{ij}\)` from `\(N(0,\sqrt{1/(T-3)})\)`
]

.column.center.image-90[![](img/corr.png)]

---

Perform hypothesis test for every _pair_ of entities, i.e., possible edge  `\(i~j\)`

We would compute `\(P\)`-value for each possible edge

When performing many independent tests, `\(P\)`-values no longer have our intended interpretation

---

### Multiple Hypothesis Testing

|     | Called Significant | Not Called Significant | Total |
|-----|--------------------|------------------------|-------|
| Null True | `\(V\)` | `\(m_0-V\)` | `\(m_0\)` |
| Altern. True | `\(S\)` | `\(m_1-S\)` | `\(m_1\)` |
| Total | `\(R\)` | `\(m-R\)` | `\(m\)` |

Note: `\(m\)` total tests

---

### Error rates

**Family-wise error rate** (FWER): the probability of at least one Type I error (false positive) `\(\mathrm{FWER}=\Pr(V\geq 1)\)`

We use Bonferroni procedure to control FWER. 

If testing at level `\(\alpha\)` (e.g., `\(\alpha=0.05\)`), only include egdes for which `\(P\)`-value `\(p_{ij} \leq \alpha/m\)`

---

### Error rates

**False Discovery Rate** (FDR): rate that false discoveries occur `\(\mathrm{FDR}=\mathbf{E}(V/R; R&gt;0)Pr(R&gt;0)\)`

We use Benjamini-Hochberg procedure to control FDR. 

Construct list of edges at FDR level `\(\beta\)` (e.g. `\(\beta=0.1\)`) if `\(p_{(k)} \leq \frac{k}{m} \beta\)`, where `\(p_{(k)}\)` is the `\(p\)`-value for the `\(k\)`-th largest `\(p\)`-value.

Note: there are other more precise FDR controlling procedures (esp. `\(q\)`-values)

---

### The problem with Pearson's correlation

Consider the following networks, where absence of edge corresponds to true _conditional independence_ between vertices in graph

.image-80.center[![](img/partial.png)]

In all three of these, Pearson's correlation test with `\(\rho_{ij}\)` is likely statistically significant.

---

Let's extend the way we think about the situation. First consider covariance _matrix_ for `\(i\)`, `\(j\)` and `\(k\)`

`$$\Sigma = \left( \begin{matrix}
\sigma_{ii}^2 &amp; \sigma_{ij} &amp; \sigma_{ik} \\
\sigma_{ji} &amp; \sigma_{jj}^2 &amp; \sigma_{jk} \\
\sigma_{ki} &amp; \sigma_{kj} &amp; \sigma_{kk}^2 
\end{matrix} \right)$$`

---

We can then think about the covariance of `\(i\)` and `\(j\)` _conditioned_ on `\(k\)`

`$$\Sigma_{ij|k} = \left( \begin{matrix} 
\sigma_{ii}^2 &amp; \sigma_{ij} \\
\sigma_{ji} &amp; \sigma_{jj}^2
\end{matrix} \right) - \sigma_{kk}^{-2}
\left( \begin{matrix}
\sigma_{ik}^2 &amp; \sigma_{ik}\sigma_{jk} \\
\sigma_{ik}\sigma_{jk} &amp; \sigma_{jk}^2
\end{matrix} \right)$$`

How do `\(y_i\)` and `\(y_j\)` co-vary around their _conditional_ means `\(E(y_i|y_k)\)` and `\(E(y_j|y_k)\)`

---

### Partial correlation networks

This leads to the concept of partial correlation (which we can derive from the conditional covariance)

`$$\rho_{ij|k} = \frac{\rho_{ij} - \rho_{ik}\rho_{jk}}{\sqrt{(1-\rho_{ik}^2)}\sqrt{(1-\rho_{jk}^2)}}$$`


---

### Partial correlation networks

What's the test now? No edge if `\(i\)` and `\(j\)` are conditionally independent (there is some `\(k\)` such that `\(\rho_{ij|k}=0\)`) 

Formally:
`$$H_0: \; \rho_{ij|k} = 0 \textrm{ for some } k \in V_{\backslash \{i,j\}}$$`
`$$H_A: \; \rho_{ij|k} \neq 0 \textrm{ for all } k$$`
---

### Partial correlation networks

To determine edge `\(i \sim j\)` compute `\(P\)`-value `\(p_{ij}\)` as 

`$$p_{ij} = \max \{ p_{ij|k}: \; k \in V_{\backslash \{i,j\}} \}$$`

where `\(p_{ij|k}\)` is a `\(P\)`-value computed from (transformed) partial correlation `\(\rho_{ij|k}\)`

Use multiple testing correction as before

---

### Problems with partial correlation networks

For every edge, must compute partial correlation wrt. every other vertex

Compound hypothesis tests like the above are harder to control for multiple testing (i.e., correction mentioned above is not quite right)

The dependence structure they represent is unclear

---

Here we turn to a very powerful abstraction, thinking of graphs as a way of describing the _joint_ distribution of gene expression measurements (Probabilistic Graphical Models).

---
layout: true

## Graphical Models

---

Consider each _complete vector_ of expression measurements at each time `\(\mathbf{y}\)`

Suppose some _conditional independence_ properties hold for some variables in `\(\mathbf{y}\)`, 
  - Example: variable `\(y_2\)` and `\(y_3\)` are independent given _remaining_ variables in `\(\mathbf{y}\)`
  
---

We can encode these conditional independence properties in a graph.

.center[![](img/lilgraph.png)]

---

Hammersley-Clifford theorem: all probability distributions that satisfy conditional independence properties in a graph can be written as

`$$P(\mathbf{y})=\frac{1}{Z}\exp \{ \sum_{c\in C} f_c(\mathbf{y}_c) \}$$`

`\(C\)` is the set of all _cliques_ in a graph, `\(c\)` a specific clique and `\(\mathbf{y_c}\)` the variables in the clique.

---

The probability distribution is determined by the choice of potential functions `\(f_c\)`. Example: 

1. `\(f_c(y_i) = -\frac{1}{2} \tau_{ii}y_i^2\)`  
2. `\(f_c(\{y_i,y_j\}) = -\frac{1}{2} \tau_{ij}y_iy_j\)`  
3. `\(f_c(\mathbf{y_c}) = 0\)` for `\(|y_c| \geq 3\)`

---
layout: true

## Graphical Gaussian Models

---
class: split-50

Define matrix `\(\Sigma^{-1}\)` as

.column[
1. `\(\Sigma_{ij}^{-1} = \tau_{ij}\)` if there is an edge between `\(y_i\)` and `\(y_j\)`  

2. `\(\Sigma_{ij}^{-1} = 0\)` otherwise

`$$\Sigma^{-1} = \left( \begin{matrix}
\tau_{11}^2 &amp; \tau_{12} &amp; 0 &amp; \tau_{14} \\
\tau_{12} &amp; \tau_{22}^2 &amp; \tau_{23} &amp; 0 \\
0 &amp; \tau_{23} &amp; \tau_{33}^2 &amp; \tau_{34} \\
\tau_{14} &amp; 0 &amp; \tau_{34} &amp; \tau_{44}^2
\end{matrix} \right)$$`

]

.column[
.center[![](img\lilgraph.png)]
]
---

With this in place, we can say that `\(\mathbf{y}\)` is distributed as _multivariate_ normal distribution `\(N(0,\Sigma)\)`.

**Connection to partial correlation:** We can think about distribution of `\(y_i\)` and `\(y_j\)` conditioned on the rest of the graph `\(V_{\backslash \{i,j\}}\)` and the (partial) correlation of `\(y_i\)` and `\(y_j\)` under this distribution

`$$\rho_{ij|V_{\backslash \{i,j\}}} = - \frac{\tau_{ij}}{\tau_{ii}\tau_{jj}}$$`

---
layout: true

## Sparse Inverse Covariance

---

With this framework in place we can now think of network structure inference.

Main idea: given draws from multivariate distribution `\(\mathbf{y}\)` (i.e., expression vector at each time point), 

Estimate a _sparse_ inverse correlation matrix, get graph from the pattern of 0's in the estimated matrix

.source[Banerjee, et al. ICML 2006, JMLR 2008, Friedman Biostatistics 2007]

---

Maximum Likelihood estimate of inverse covariance is given by solution to

`$$\max_{X \succ 0} \log \det X - (SX)$$`

`\(S\)` is the estimated sample covariance matrix

`$$S=\sum_{t=1}^T \mathbf{y_t}\mathbf{y_t}'$$`

(Yuck)

---

We can induce zeros in the solution using a penalized likelihood estimate

`$$\max_{X \succ 0} \log \det X - (SX) - \lambda \|X\|_1$$`

where `$$\|X\|_1 = \sum_{ij} X_{ij}$$`

(Yuckier)

---
class: split-50

### Block-coordinate ascent

Solve by maximizing one column of matrix at a time (edges for each variable, e.g., `\(x_{12}\)` below)

`$$\min_{\beta} \frac{1}{2} \|X_{11}^{1/2}\beta - z\|^2 + \lambda \|\beta\|_1$$`

with `\(z=W_{11}^{1/2}s_{12}\)` and

.column[
`$$X=\left( \begin{matrix} 
X_{11} &amp; x_{12} \\
x_{12} &amp; x_{22}
\end{matrix} \right)$$`
]

.column[
`$$S=\left( \begin{matrix} 
S_{11} &amp; s_{12} \\
s_{12} &amp; s_{22}
\end{matrix} \right)$$`
]

---

### Block-coordinate ascent

Solve by maximizing one column of matrix at a time (edges for each variable, e.g., `\(x_{12}\)` below)

`$$\min_{\beta} \frac{1}{2} \|X_{11}^{1/2}\beta - z\|^2 + \lambda \|\beta\|_1$$`

Solution is then `\(x_{12} = W_{11}\beta\)`

(This is l1-regularized least squares, easy to solve, not yucky at all)

---

### Block-coordinate ascent

Solve by maximizing one column of matrix at a time (edges for each variable, e.g., `\(x_{12}\)` below)

`$$\min_{\beta} \frac{1}{2} \|X_{11}^{1/2}\beta - z\|^2 + \lambda \|\beta\|_1$$`

Iterate over columns of matrix until `\(\beta\)` converges (or even better, until objective function converges)

---
layout: false

## Summary

Using Gaussian Graphical Model representation

- multivariate normal probability over a sparse graph
- take resulting graph as e.g., _gene network_

Use sparsity-inducing regularization (l1-norm)

Block-coordinate ascent method leads to l1-regularized regression at each step

- Can use efficient coordinate descent (soft-thresholding) to solve regression problem
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
